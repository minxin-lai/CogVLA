# ğŸ“ CogVLA è®­ç»ƒå…¨æµç¨‹æ·±åº¦è§£æ

æœ¬æ–‡æ¡£ä»¥**è®­ç»ƒå…¨æµç¨‹**ä¸ºä¸»çº¿ï¼Œè®²è§£CogVLAçš„å››å¤§æ ¸å¿ƒä¼˜åŒ–æ¨¡å—å¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ååŒå·¥ä½œã€‚

---

## ğŸ“‹ ç›®å½•

1. [é˜¶æ®µä¸€ï¼šå¯åŠ¨ä¸é…ç½®](#é˜¶æ®µä¸€å¯åŠ¨ä¸é…ç½®)
2. [é˜¶æ®µäºŒï¼šæ¨¡å‹åˆå§‹åŒ–](#é˜¶æ®µäºŒæ¨¡å‹åˆå§‹åŒ–)
3. [é˜¶æ®µä¸‰ï¼šLoRAæ³¨å…¥ä¸å‚æ•°è§£å†»](#é˜¶æ®µä¸‰loraæ³¨å…¥ä¸å‚æ•°è§£å†»)
4. [é˜¶æ®µå››ï¼šVision Backbone Wrapperæ³¨å…¥](#é˜¶æ®µå››vision-backbone-wrapperæ³¨å…¥)
5. [é˜¶æ®µäº”ï¼šå‰å‘ä¼ æ’­](#é˜¶æ®µäº”å‰å‘ä¼ æ’­)
6. [é˜¶æ®µå…­ï¼šCheckpointä¿å­˜](#é˜¶æ®µå…­checkpointä¿å­˜)
7. [é˜¶æ®µä¸ƒï¼šæ¨ç†åŠ è½½](#é˜¶æ®µä¸ƒæ¨ç†åŠ è½½)
8. [å®Œæ•´æ•°æ®æµæ€»ç»“](#å®Œæ•´æ•°æ®æµæ€»ç»“)

---

## é˜¶æ®µä¸€ï¼šå¯åŠ¨ä¸é…ç½®

### 1.1 å‘½ä»¤è¡Œå¯åŠ¨

```bash
torchrun --nproc_per_node=8 vla-scripts/finetune.py \
    --vla_path openvla/openvla-7b \
    --use_film True \
    --use_aggr True \
    --num_vision_aggr 64 \
    --use_lfp True \
    --lfp_enable_film True \
    --vision_aggregate_type moe \
    --dataset_name aloha_scoop_food
```

### 1.2 é…ç½®æ³¨å…¥åˆ°æ¨¡å‹

**å…³é”®ä»£ç ä½ç½®**ï¼š`vla-scripts/finetune.py:931-942`

è¿™æ˜¯**å…³é”®ç¬¬ä¸€æ­¥**ï¼šå°†CogVLAçš„4ä¸ªæ¨¡å—å¼€å…³å†™å…¥é…ç½®

```python
# åŠ è½½base OpenVLAé…ç½®
vla_cfg = AutoConfig.from_pretrained(cfg.vla_path)

# ====== æ¨¡å—3ï¼šLFPé…ç½®æ³¨å…¥ ======
vla_cfg.text_config.use_lfp = True              # å¯ç”¨LFP
vla_cfg.text_config.lfp_average_factor = 0.5    # å¹³å‡ä¿ç•™50%
vla_cfg.text_config.lfp_type = "shiftedcos_decay_0.85_0.15"
vla_cfg.text_config.lfp_enable_film = True      # LFPä¸­å¯ç”¨FiLM

# ====== æ¨¡å—2ï¼šAggregation Tokensé…ç½® ======
vla_cfg.use_aggr = True                         # å¯ç”¨å‹ç¼©
vla_cfg.use_film = True                         # é…åˆFiLM
vla_cfg.num_vision_aggr = 64                    # å‹ç¼©åˆ°64ä¸ªtoken

# ====== æ¨¡å—1ï¼šMoE Routeré…ç½® ======
vla_cfg.vision_aggregate_type = 'moe'           # MoEèåˆæ¨¡å¼
```

**ğŸ”¥ è¿™é‡Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**

å½“è°ƒç”¨`AutoModelForVision2Seq.from_pretrained(vla_path, config=vla_cfg)`æ—¶ï¼Œæ¨¡å‹ä¼šæ ¹æ®è¿™äº›å¼€å…³åˆå§‹åŒ–å¯¹åº”ç»„ä»¶ã€‚

---

## é˜¶æ®µäºŒï¼šæ¨¡å‹åˆå§‹åŒ–

### 2.1 AutoModelåŠ è½½è§¦å‘ç»„ä»¶åˆ›å»º

**å…³é”®ä»£ç ä½ç½®**ï¼š`vla-scripts/finetune.py:946-952`

```python
vla = AutoModelForVision2Seq.from_pretrained(
    cfg.vla_path,
    config=vla_cfg,  # åŒ…å«CogVLAé…ç½®
    trust_remote_code=True
)
```

**å†…éƒ¨å‘ç”Ÿçš„äº‹æƒ…**ï¼ˆåœ¨`prismatic/extern/hf/modeling_prismatic.py`çš„`__init__`ä¸­ï¼‰ï¼š

```python
def __init__(self, config: PrismaticConfig):
    # 1ï¸âƒ£ å¦‚æœconfig.text_config.use_lfpä¸ºTrue
    if config.text_config.use_lfp:
        replace_llama_forward()  # Monkey-patch LLMï¼Œæ³¨å…¥LFP layers
        
    # 2ï¸âƒ£ åˆ›å»ºbase LLMï¼ˆå·²è¢«monkey-patchï¼‰
    self.language_model = AutoModelForCausalLM.from_config(config.text_config)
    
    # 3ï¸âƒ£ å¦‚æœvision_aggregate_type == 'moe'
    if config.vision_aggregate_type == 'moe':
        # åˆ›å»ºMoE Router
        self.aggregation_router = MoEAggregator(
            num_experts=2,
            seq_dim=config.llm_backbone_config.hidden_size
        )
        # ä¸ºä¸¤ä¸ªä¸“å®¶å„åˆ›å»ºæŠ•å½±å±‚
        self.featurizer_proj = nn.Linear(...)      # SigLIPæŠ•å½±
        self.fused_featurizer_proj = nn.Linear(...)  # DINOv2æŠ•å½±
```

### 2.2 æ¨¡å‹ç»“æ„

**æ­¤æ—¶æ¨¡å‹ç»“æ„**ï¼š

```
vla
â”œâ”€â”€ vision_backbone (åŸå§‹ï¼šä¸¤ä¸ªç‹¬ç«‹ViT)
â”‚   â”œâ”€â”€ featurizer (SigLIP)
â”‚   â””â”€â”€ fused_featurizer (DINOv2)
â”œâ”€â”€ aggregation_router (MoE Router) â† æ–°å¢
â”œâ”€â”€ featurizer_proj â† æ–°å¢
â”œâ”€â”€ fused_featurizer_proj â† æ–°å¢
â””â”€â”€ language_model (Llama)
    â”œâ”€â”€ layers[0-31]
    â”‚   â”œâ”€â”€ layers[5,10,15,20,25,30] â†’ LlamaDecoderLFPLayer â† è¢«monkey-patchæ›¿æ¢
    â”‚   â”‚   â””â”€â”€ router: FiLMedTokenRouter â† æ–°å¢
    â”‚   â””â”€â”€ å…¶ä»–å±‚ï¼šæ ‡å‡†LlamaDecoderLayer
    â””â”€â”€ sdpa_attention â†’ llama_sdpa_attention_forward â† è¢«å…¨å±€monkey-patch
```

---

## é˜¶æ®µä¸‰ï¼šLoRAæ³¨å…¥ä¸å‚æ•°è§£å†»

### 3.1 LoRAé…ç½®ï¼šæ’é™¤CogVLAç»„ä»¶

**å…³é”®ä»£ç ä½ç½®**ï¼š`vla-scripts/finetune.py:958-969`

```python
lora_config = LoraConfig(
    r=32,
    target_modules=find_all_linear_names(
        vla, 
        excluded_names=[
            'featurizer_proj',        # MoEç»„ä»¶
            'fused_featurizer_proj',  # MoEç»„ä»¶
            'aggregation_router',     # MoEç»„ä»¶
            'router',                 # LFP Router
            'scale', 'shift'          # FiLM components
        ]
    ),
)
vla = get_peft_model(vla, lora_config)
```

**ğŸ¤” ä¸ºä»€ä¹ˆæ’é™¤è¿™äº›ï¼Ÿ**

LoRAåªé€‚ç”¨äºåŸå§‹çš„å¤§å‹çº¿æ€§å±‚ï¼ˆå¦‚attentionã€MLPï¼‰ã€‚CogVLAçš„æ–°å¢ç»„ä»¶ï¼š
- **MoE Router**ï¼šå°å‹MLPï¼Œéœ€è¦å…¨é‡è®­ç»ƒ
- **LFP Router**ï¼šæ¯å±‚éƒ½æœ‰ï¼Œéœ€è¦å…¨é‡è®­ç»ƒ
- **FiLM scale/shift**ï¼šåŠ¨æ€è°ƒåˆ¶å‚æ•°ï¼Œéœ€è¦å…¨é‡è®­ç»ƒ

### 3.2 æ˜¾å¼è§£å†»CogVLAç»„ä»¶

**å…³é”®ä»£ç ä½ç½®**ï¼š`vla-scripts/finetune.py:971-984`

```python
# è§£å†»MoE RouteråŠå…¶æŠ•å½±å±‚
if hasattr(vla.model, 'aggregation_router'):
    for n, p in vla.model.aggregation_router.named_parameters():
        p.requires_grad = True
    for n, p in vla.model.featurizer_proj.named_parameters():
        p.requires_grad = True
    for n, p in vla.model.fused_featurizer_proj.named_parameters():
        p.requires_grad = True

# è§£å†»æ‰€æœ‰LFP Routers
if vla.config.text_config.use_lfp:
    for n, p in vla.model.language_model.named_parameters():
        if 'router' in n:  # åŒ¹é…æ‰€æœ‰å±‚çš„router
            p.requires_grad = True
```

### 3.3 è®­ç»ƒå‚æ•°åˆ†å¸ƒ

**æ­¤æ—¶è®­ç»ƒå‚æ•°åˆ†å¸ƒ**ï¼š

```
LoRAå‚æ•°ï¼ˆæ¢¯åº¦æ›´æ–°ï¼‰:
  - language_modelçš„attention q_proj, k_proj, v_proj, o_proj
  - language_modelçš„MLP gate_proj, up_proj, down_proj
  
å…¨é‡è®­ç»ƒå‚æ•°ï¼ˆæ¢¯åº¦æ›´æ–°ï¼‰:
  - aggregation_router: ~131Kå‚æ•°
  - featurizer_proj: ~4Må‚æ•°
  - fused_featurizer_proj: ~5Må‚æ•°
  - language_model.layers[*].router: ~32K Ã— 6å±‚ = ~192Kå‚æ•°
  
å†»ç»“å‚æ•°ï¼ˆä¸æ›´æ–°ï¼‰:
  - vision_backboneçš„æ‰€æœ‰å‚æ•°
  - language_modelçš„embeddingsã€layernormç­‰
```

---

## é˜¶æ®µå››ï¼šVision Backbone Wrapperæ³¨å…¥

> ğŸ“– **è¯¦ç»†è¯´æ˜**ï¼šå…³äºVision Backboneçš„å®Œæ•´æ‰§è¡Œæµç¨‹ï¼ˆåŒ…æ‹¬aggregation tokenså’ŒFiLMè°ƒåˆ¶æœºåˆ¶ï¼‰ï¼Œè¯·å‚è€ƒï¼š[vision_backbone_flow.md](./vision_backbone_flow.md)

### 4.1 è¿è¡Œæ—¶Wrapperæ³¨å…¥

**å…³é”®ä»£ç ä½ç½®**ï¼š`vla-scripts/finetune.py:990-1021`

```python
if cfg.use_aggr:
    # é€‰æ‹©wrapperç±»
    wrapper_class = (FiLMedPrismaticVisionBackboneAggregator  # use_film=True
                     if cfg.use_film 
                     else PrismaticVisionBackboneAggregator)  # use_film=False
    
    # âš ï¸ æ³¨æ„ï¼šè¿™é‡Œç”¨vla.model.è€Œä¸æ˜¯vla.
    # å› ä¸ºvlaå·²è¢«LoRAåŒ…è£¹ï¼Œå¿…é¡»ä¿®æ”¹å†…éƒ¨base_model
    vla.model.vision_backbone = wrapper_class(
        vision_backbone=vla.model.vision_backbone,  # åŸå§‹backbone
        llm_dim=4096,
        num_vision_aggr=64
    )
```

### 4.2 Wrapperå†…éƒ¨æœºåˆ¶

**å…³é”®ä»£ç ä½ç½®**ï¼š`prismatic/models/vit_wrapper_reg.py`

```python
class FiLMedPrismaticVisionBackboneAggregator:
    def __init__(self, vision_backbone, llm_dim, num_vision_aggr):
        self.vision_backbone = vision_backbone  # ä¿ç•™åŸå§‹
        
        # 1ï¸âƒ£ åˆ›å»ºå¯è®­ç»ƒçš„èšåˆtokens
        self.vision_aggr_featurizer = nn.Parameter(
            torch.randn(1, num_vision_aggr, embed_dim)  # [1,64,1024]
        )
        
        # 2ï¸âƒ£ Monkey-patch ViTï¼šæ³¨å…¥aggregation tokens
        self._wrap_vit(vision_backbone.featurizer, self.vision_aggr_featurizer)
        
        if use_fused:
            self.vision_aggr_fused_featurizer = nn.Parameter(...)
            self._wrap_vit(vision_backbone.fused_featurizer, ...)
    
    def _wrap_vit(self, vit, vision_aggr):
        # A. ç”¨FiLM wrapperåŒ…è£¹æ¯ä¸ªTransformer block
        for block in vit.blocks:
            block_wrapper = FiLMedVisionTransformerBlock(
                block=block,
                vision_dim=vit.num_features,
                llm_dim=self.llm_dim
            )
        
        # B. æ›¿æ¢ViTçš„forwardæ–¹æ³•
        vit.forward = partial(vit.get_intermediate_layers, vision_aggr=vision_aggr)
```

### 4.3 Vision Backboneç»“æ„å˜åŒ–

**ç°åœ¨Vision Backboneç»“æ„å˜æˆ**ï¼š

```
vision_backbone (Wrapper)
â”œâ”€â”€ vision_backbone (åŸå§‹)
â”‚   â”œâ”€â”€ featurizer (SigLIP ViT) â† è¢«monkey-patch
â”‚   â”‚   â”œâ”€â”€ blocks[0-26]  â†’ æ¯å±‚åŒ…è£¹äº†FiLMedBlock
â”‚   â”‚   â””â”€â”€ forward â†’ æ”¹ä¸ºè¾“å‡ºaggr_tokens
â”‚   â””â”€â”€ fused_featurizer (DINOv2 ViT) â† è¢«monkey-patch
â”œâ”€â”€ vision_aggr_featurizer [1,64,1024] â† å¯è®­ç»ƒå‚æ•°
â””â”€â”€ vision_aggr_fused_featurizer [1,64,1152] â† å¯è®­ç»ƒå‚æ•°
```

---

## é˜¶æ®µäº”ï¼šå‰å‘ä¼ æ’­

### 5.1 å®Œæ•´æ•°æ®æµè¿½è¸ª

**å‡è®¾è¾“å…¥**ï¼š
- å›¾åƒï¼š3å¼ ï¼ˆprimary + 2 wristï¼‰
- æ–‡æœ¬ï¼š"æŠŠè‹¹æœæ”¾åˆ°ç¢—é‡Œ"
- åŠ¨ä½œï¼š7æ­¥è½¨è¿¹

### Step 1: å›¾åƒç¼–ç ï¼ˆå¸¦Aggregation Tokensï¼‰

> ğŸ’¡ **è¯¦ç»†æœºåˆ¶è¯´æ˜**ï¼šå…³äºvision backboneå†…éƒ¨å¦‚ä½•å®ç°256â†’64 tokenå‹ç¼©å’ŒFiLMè°ƒåˆ¶ï¼Œè¯·å‚è€ƒï¼š[vision_backbone_flow.md](./vision_backbone_flow.md)

**å…³é”®ä»£ç ä½ç½®**ï¼š`prismatic/extern/hf/modeling_prismatic.py:_process_vision_features()`

```python
# åœ¨modeling_prismatic.py:forward()ä¸­
pixel_values = batch["pixel_values"]  # [B, 3*6, 224, 224]
language_embeddings = self.get_input_embeddings()(input_ids)  # [B, 20, 4096]

# è°ƒç”¨è¢«wrapperçš„vision backbone
patch_features = self.vision_backbone(pixel_values, language_embeddings)
# å†…éƒ¨æµç¨‹:
#  1. æ–‡æœ¬å‹ç¼©: mean(dim=1) â†’ [B, 4096]
#  2. åˆ†ç¦»3å¼ å›¾: æ¯å¼ 6é€šé“ â†’ SigLIP(3ch) + DINOv2(3ch)
#  3. æ¯ä¸ªViT: 256 patches â†’ æ‹¼æ¥64ä¸ªaggr_tokens â†’ Transformer â†’ è¾“å‡º64ä¸ªtokens
#  4. FiLMè°ƒåˆ¶: æ¯å±‚ç”¨æ–‡æœ¬ç”Ÿæˆgamma/betaè°ƒåˆ¶è§†è§‰ç‰¹å¾
```

**è¾“å‡ºæ ¼å¼**ï¼š`[(siglip1, dino1), (siglip2, dino2), (siglip3, dino3)]`
- æ¯ä¸ªtuple: `([B,64,1024], [B,64,1152])`
- 3å¼ å›¾ Ã— 2ç¼–ç å™¨ = 6ç»„å‹ç¼©åçš„è§†è§‰ç‰¹å¾

### Step 2: MoEèåˆ

**å…³é”®ä»£ç ä½ç½®**ï¼š`prismatic/extern/hf/modeling_prismatic.py:_aggregate_patch_features()`

```python
# å›åˆ°modeling_prismatic.py:_process_vision_features()
all_image_embeds = []
for img_patches in patch_features:  # éå†3å¼ å›¾
    # è°ƒç”¨èšåˆå‡½æ•°
    image_embeds = self._aggregate_patch_features(
        img_patches,  # (siglip_patches, dino_patches)
        language_embeddings
    )
    all_image_embeds.append(image_embeds)
```

**_aggregate_patch_featureså†…éƒ¨**ï¼š

```python
def _aggregate_patch_features(self, patch_features, language_embeddings):
    if self.config.vision_aggregate_type == 'moe':
        # 1. æå–ä¸¤ä¸ªä¸“å®¶çš„features
        patches_siglip, patches_dino = patch_features  # [B,64,1024], [B,64,1152]
        
        # 2. æŠ•å½±åˆ°LLMç»´åº¦
        proj_siglip = self.featurizer_proj(patches_siglip)      # [B,64,4096]
        proj_dino = self.fused_featurizer_proj(patches_dino)    # [B,64,4096]
        
        # 3. MoE Routerå†³ç­–
        avg_lang = language_embeddings.mean(dim=1)  # [B, 4096]
        fused = self.aggregation_router(
            [proj_siglip, proj_dino],  # ä¸¤ä¸ªä¸“å®¶
            avg_lang                    # æ–‡æœ¬condition
        )
        # Routerå†…éƒ¨ï¼š
        #   ratios = softmax(MLP(avg_lang))  # [B, 2] ä¾‹å¦‚ï¼š[0.7, 0.3]
        #   output = ratios[0] * proj_siglip + ratios[1] * proj_dino
        
        return fused  # [B, 64, 4096]
```

**æ‹¼æ¥3å¼ å›¾çš„ç»“æœ**ï¼š

```python
image_embeds = torch.cat(all_image_embeds, dim=1)  # [B, 192, 4096] (64*3)
```

### Step 3: æ„å»ºå¤šæ¨¡æ€åºåˆ—

```python
# æ–‡æœ¬embedding
input_embeddings = self.get_input_embeddings()(input_ids)  # [B, seq_len, 4096]

# æ’å…¥è§†è§‰tokensåˆ°BOSåé¢
multimodal_embeddings = torch.cat([
    input_embeddings[:, :1, :],      # [BOS]
    image_embeds,                     # 192ä¸ªè§†è§‰tokens
    input_embeddings[:, 1:, :]       # æ–‡æœ¬ + åŠ¨ä½œtokens
], dim=1)  # [B, 1+192+20+43, 4096] = [B, 256, 4096]
```

**åºåˆ—å¸ƒå±€**ï¼š

```
[BOS] [V1...V192] [æ‹¿èµ·è‹¹æœæ”¾åˆ°ç¢—é‡Œ] [A1...A43_STOP]
  â†‘      â†‘              â†‘                    â†‘
 token  è§†è§‰           æ–‡æœ¬æŒ‡ä»¤            åŠ¨ä½œåºåˆ—
  0     1-192         193-212           213-255
```

### Step 4: LLMå¤„ç†ï¼ˆå¸¦LFPå‰ªæï¼‰

> ğŸ“– **è¯¦ç»†æœºåˆ¶è¯´æ˜**ï¼šå…³äºLFPå‰ªæçš„å®Œæ•´æœºåˆ¶ï¼ˆæ‰“åˆ†ã€é€‰æ‹©ã€å‹ç¼©ã€æ¢å¤æµç¨‹åŠåŠ é€ŸåŸç†ï¼‰ï¼Œè¯·å‚è€ƒï¼š[LFP_mechanism.md](./LFP_mechanism.md)

**å…³é”®ä»£ç ä½ç½®**ï¼š`prismatic/models/modeling_llama.py:LlamaDecoderLFPLayer.forward()`

```python
outputs = self.language_model(
    inputs_embeds=multimodal_embeddings,
    attention_mask=attention_mask,
    ...
)
```

**åœ¨LLMå†…éƒ¨**ï¼ˆç»è¿‡monkey-patchï¼‰ï¼š

```python
# Layer 0-4: æ ‡å‡†LlamaDecoderLayer
hidden = standard_layers[0-4](hidden)  # æ‰€æœ‰256ä¸ªtokens

# Layer 5: LlamaDecoderLFPLayerï¼ˆç¬¬1ä¸ªå‰ªæå±‚ï¼‰
def forward(self, hidden_states):  # [B, 256, 4096]
    # 1. Routeræ‰“åˆ†
    if self.config.lfp_enable_film:
        # FiLMed Routerï¼šç”¨æ–‡æœ¬è°ƒåˆ¶è§†è§‰
        router_logits = self.router(hidden_states, attn_mask, num_vision=192)
    else:
        router_logits = self.router(hidden_states)  # [B, 256, 2]
    
    keep_probs = softmax(router_logits)[:, :, 1]  # [B, 256]
    
    # 2. å¼ºåˆ¶ä¿ç•™éè§†è§‰tokens
    force_mask = torch.zeros_like(keep_probs)
    force_mask[:, 0] = inf              # ä¿ç•™BOS
    force_mask[:, 193:] = inf           # ä¿ç•™æ–‡æœ¬+åŠ¨ä½œ
    
    # 3. Top-Ké€‰æ‹©
    router_factor = shifted_cos(layer_idx=5)  # ä¾‹å¦‚0.85
    keep_len = 1 + int(192*0.85) + 20 + 43 = 227
    
    _, indices = topk(keep_probs + force_mask, k=227)
    # indicesä¾‹å¦‚ï¼š[0, 1, 3, 7, ..., 150, 193, 194, ..., 255]
    # ä¸¢å¼ƒäº†çº¦30ä¸ªä¸é‡è¦çš„è§†è§‰tokens
    
    # 4. Gatherä¿ç•™çš„tokens
    kept_hidden = gather(hidden_states, indices)  # [B, 227, 4096]
    
    # 5. é‡å»ºattention maskï¼ˆå…³é”®ï¼ï¼‰
    kept_mask = gather(gather(attn_mask, indices, dim=2), indices, dim=3)
    
    # 6. æ ‡å‡†transformerå±‚
    output = super().forward(kept_hidden, kept_mask, ...)
    
    # 7. Scatterå›åŸä½ç½®
    hidden_states = scatter(hidden_states, indices, output)  # [B, 256, 4096]
    
    return hidden_states

# Layer 6-9: æ ‡å‡†å±‚
# Layer 10: LFP Layer (router_factor=0.75) â†’ ä¿ç•™144ä¸ªè§†è§‰
# ...
# Layer 30: LFP Layer (router_factor=0.20) â†’ ä¿ç•™38ä¸ªè§†è§‰
# Layer 31: æ ‡å‡†å±‚
```

**å‰ªæè¿›åº¦å¯è§†åŒ–**ï¼š

```
Layer 0:  [BOS] + 192è§†è§‰ + 20æ–‡æœ¬ + 43åŠ¨ä½œ = 256 tokens
Layer 5:  [BOS] + 163è§†è§‰ + 20æ–‡æœ¬ + 43åŠ¨ä½œ = 227 tokens (ä¸¢29ä¸ª)
Layer 10: [BOS] + 144è§†è§‰ + 20æ–‡æœ¬ + 43åŠ¨ä½œ = 208 tokens (ä¸¢48ä¸ª)
Layer 15: [BOS] + 115è§†è§‰ + 20æ–‡æœ¬ + 43åŠ¨ä½œ = 179 tokens (ä¸¢77ä¸ª)
Layer 20: [BOS] + 77è§†è§‰  + 20æ–‡æœ¬ + 43åŠ¨ä½œ = 141 tokens (ä¸¢115ä¸ª)
Layer 25: [BOS] + 57è§†è§‰  + 20æ–‡æœ¬ + 43åŠ¨ä½œ = 121 tokens (ä¸¢135ä¸ª)
Layer 30: [BOS] + 38è§†è§‰  + 20æ–‡æœ¬ + 43åŠ¨ä½œ = 102 tokens (ä¸¢154ä¸ª)
```

### Step 5: Parallel Action Chunkingï¼ˆSDPAå±‚ï¼‰

**å…³é”®ä»£ç ä½ç½®**ï¼š`prismatic/models/modeling_llama.py:llama_sdpa_attention_forward()`

åœ¨æ¯ä¸ªattentionå±‚ä¸­ï¼š

```python
# llama_sdpa_attention_forwardï¼ˆè¢«å…¨å±€monkey-patchï¼‰

# æ ‡å‡†QKVè®¡ç®—
Q, K, V = self.q_proj(hidden), self.k_proj(hidden), self.v_proj(hidden)

# æ„å»ºcausal mask
causal_mask = ... # æ ‡å‡†ä¸‹ä¸‰è§’mask [B, 1, seq, seq]

# ========== CATTENä¿®æ”¹ ==========
# æ‰¾åˆ°action tokensåŒºåŸŸå¹¶æ¸…é›¶mask
num_act = 43
for idx in range(batch_size):
    # æ¸…é›¶æœ€å43ä¸ªtokensä¹‹é—´çš„mask
    causal_mask[idx, :, -43:, -43:] = 0

# æ ‡å‡†çš„ä½ç½®ï¼š
#   A1åªèƒ½çœ‹: [BOS, V, T, A1]
#   A2åªèƒ½çœ‹: [BOS, V, T, A1, A2]
# ä¿®æ”¹åï¼š
#   A1èƒ½çœ‹: [BOS, V, T, A1, A2, ..., A43]  â† çœ‹åˆ°æ‰€æœ‰åŠ¨ä½œ
#   A2èƒ½çœ‹: [BOS, V, T, A1, A2, ..., A43]  â† åŒæ ·
# ================================

attn_out = scaled_dot_product_attention(Q, K, V, mask=causal_mask)
```

**æ•ˆæœå¯¹æ¯”**ï¼š

```
åŸå§‹attention pattern:
  BOS  V1  ...  T20  A1  A2  A3
A1  1   1   ...  1   1   0   0
A2  1   1   ...  1   1   1   0
A3  1   1   ...  1   1   1   1

ä¿®æ”¹åï¼ˆParallel Action):
  BOS  V1  ...  T20  A1  A2  A3
A1  1   1   ...  1   1   1   1  â† èƒ½çœ‹åˆ°A2,A3!
A2  1   1   ...  1   1   1   1  â† ä¸‰ä¸ªåŠ¨ä½œäº’ç›¸å¯è§
A3  1   1   ...  1   1   1   1
```

### Step 6: æŸå¤±è®¡ç®—

```python
# è·å–æœ€åä¸€å±‚hidden states
last_hidden = outputs.hidden_states[-1]  # [B, 256, 4096]

# æå–åŠ¨ä½œéƒ¨åˆ† (indices 213-255)
action_hidden = last_hidden[:, 213:256, :]  # [B, 43, 4096]

# é€šè¿‡LM headé¢„æµ‹
logits = self.lm_head(last_hidden)  # [B, 256, vocab_size]

# è®¡ç®—äº¤å‰ç†µ
loss = cross_entropy(logits, labels, ignore_index=-100)
```

---

## é˜¶æ®µå…­ï¼šCheckpointä¿å­˜

### 6.1 ä¿å­˜ç­–ç•¥çš„å…³é”®è®¾è®¡

**å…³é”®ä»£ç ä½ç½®**ï¼š`vla-scripts/finetune.py:631-753`

```python
def save_training_checkpoint(...):
    checkpoint_dir = run_dir / f"step_{log_step}"
    adapter_dir = checkpoint_dir / "lora_adapter"
    
    # 1ï¸âƒ£ ä¿å­˜LoRA adapter (æ€»æ˜¯)
    vla.module.save_pretrained(adapter_dir)
    
    # 2ï¸âƒ£ ä¿å­˜Vision Backbone (å¦‚æœuse_aggræˆ–use_film)
    if vla_config.use_film or vla_config.use_aggr:
        torch.save(
            vla.module.vision_backbone.state_dict(),
            checkpoint_dir / "vision_backbone--checkpoint.pt"
        )
    
    # 3ï¸âƒ£ ä¿å­˜éLoRAå¯è®­ç»ƒå‚æ•°
    if cfg.merge_lora_during_training == False:
        non_lora_trainables = get_peft_state_non_lora(
            vla.named_parameters(),
            excluded_names=['vision_backbone']
        )
        # åŒ…å«ï¼š
        #  - aggregation_router
        #  - featurizer_proj
        #  - fused_featurizer_proj
        #  - language_model.layers[*].router
        
        torch.save(
            non_lora_trainables,
            checkpoint_dir / "non_lora_trainables--checkpoint.pt"
        )
    
    # 4ï¸âƒ£ ä¿å­˜æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    save_dataset_statistics(
        train_dataset.dataset_statistics,
        checkpoint_dir / "dataset_statistics.json"
    )
```

### 6.2 Checkpointç›®å½•ç»“æ„

**ä¿å­˜åçš„ç›®å½•ç»“æ„**ï¼š

```
runs/step_10000/
â”œâ”€â”€ lora_adapter/
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””â”€â”€ adapter_model.safetensors  # LoRAæƒé‡
â”œâ”€â”€ vision_backbone--checkpoint.pt   # Wrapper + aggr_tokens
â”œâ”€â”€ non_lora_trainables--checkpoint.pt  # MoE Router + LFP Routers
â”œâ”€â”€ dataset_statistics.json
â”œâ”€â”€ config.json
â”œâ”€â”€ preprocessor_config.json
â””â”€â”€ tokenizer.json
```

### 6.3 ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

**é—®é¢˜**ï¼šLoRAä¸åŒ…å«å…¨éƒ¨è®­ç»ƒå‚æ•°

```
ä¿å­˜çš„LoRA adapteråªåŒ…å«ï¼š
  âœ“ attentionçš„q_proj, k_proj, v_proj, o_projçš„LoRAçŸ©é˜µ
  âœ“ MLPçš„gate_proj, up_proj, down_projçš„LoRAçŸ©é˜µ

ç¼ºå¤±ï¼š
  âœ— aggregation_router (å…¨é‡è®­ç»ƒ)
  âœ— featurizer_proj/fused_featurizer_proj (å…¨é‡è®­ç»ƒ)
  âœ— language_model.layers[*].router (å…¨é‡è®­ç»ƒ)
  âœ— vision_backboneçš„wrapperå’Œaggr_tokens
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. `vision_backbone--*.pt` å•ç‹¬ä¿å­˜ï¼ˆå› ä¸ºwrapperä¸åœ¨base modelé‡Œï¼‰
2. `non_lora_trainables--*.pt` å•ç‹¬ä¿å­˜ï¼ˆMoE + LFP routersï¼‰
3. æ¨ç†æ—¶éœ€è¦ä¸‰æ­¥åŠ è½½ï¼ˆè§ä¸‹ä¸€é˜¶æ®µï¼‰

---

## é˜¶æ®µä¸ƒï¼šæ¨ç†åŠ è½½

### 7.1 å®Œæ•´åŠ è½½æµç¨‹

**å…³é”®ä»£ç ä½ç½®**ï¼š`experiments/robot/openvla_utils.py:get_vla()`

```python
def get_vla(cfg):
    # Step 1: åŠ è½½merged modelï¼ˆbase + LoRA mergedï¼‰
    vla = AutoModelForVision2Seq.from_pretrained(
        cfg.pretrained_checkpoint,
        trust_remote_code=True
    )
    # æ­¤æ—¶æœ‰ï¼šbase model + merged LoRA
    # ç¼ºå°‘ï¼švision wrapper, MoE router, LFP routers
    
    # Step 2: å¦‚æœuse_aggræˆ–use_filmï¼Œé‡å»ºwrapper
    if vla.config.use_film or vla.config.use_aggr:
        # 2.1 åˆ›å»ºwrapper
        if vla.config.use_aggr:
            wrapper_class = (FiLMedPrismaticVisionBackboneAggregator
                           if vla.config.use_film
                           else PrismaticVisionBackboneAggregator)
            vla.vision_backbone = wrapper_class(
                vision_backbone=vla.vision_backbone,
                llm_dim=vla.llm_dim,
                num_vision_aggr=vla.config.num_vision_aggr
            )
        
        # 2.2 åŠ è½½ä¿å­˜çš„wrapperæƒé‡
        vision_state = torch.load(
            checkpoint_dir / "vision_backbone--checkpoint.pt"
        )
        vla.vision_backbone.load_state_dict(vision_state)
    
    # Step 3: åŠ è½½dataset statisticsï¼ˆç”¨äºåŠ¨ä½œunnormalizeï¼‰
    stats_path = checkpoint_dir / "dataset_statistics.json"
    vla.norm_stats = json.load(open(stats_path))
    
    return vla
```

### 7.2 æ¨ç†å‰å‘ä¼ æ’­

```python
def get_vla_action(vla, image, instruction):
    # 1. å‡†å¤‡è¾“å…¥
    prompt = f"In: {instruction}?\nOut:"
    inputs = processor(prompt, image)
    
    # 2. VLAæ¨ç†
    with torch.no_grad():
        output = vla.predict_action(
            input_ids=inputs['input_ids'],
            pixel_values=inputs['pixel_values'],
            unnorm_key='aloha_scoop_food'  # ç”¨äºunnormalize
        )
    
    # 3. UnnormalizeåŠ¨ä½œ
    normalized_actions = output  # [-1, 1]
    actions = vla._unnormalize_actions(
        normalized_actions,
        unnorm_key='aloha_scoop_food'
    )
    # ä½¿ç”¨dataset_statistics.jsonä¸­çš„min/maxè¿˜åŸ
    
    return actions  # çœŸå®æœºå™¨äººåŠ¨ä½œç©ºé—´
```

---

## å®Œæ•´æ•°æ®æµæ€»ç»“

### è®­ç»ƒæ—¶çš„å®Œæ•´å‰å‘ä¼ æ’­

```
å›¾åƒ [B,3*6,224,224]
  â†“
Vision Backbone (Wrapper)
  â”œâ†’ ViT + Aggr Tokens â†’ [B,64,1024] (SigLIP)
  â””â†’ ViT + Aggr Tokens â†’ [B,64,1152] (DINOv2)
  â†“
MoE Router (ç”¨æ–‡æœ¬condition)
  Routerå†³ç­–: 0.7*SigLIP + 0.3*DINOv2
  â†“
æ‹¼æ¥3å¼ å›¾ â†’ [B,192,4096]
  â†“
æ„å»ºå¤šæ¨¡æ€åºåˆ—
  [BOS] + 192è§†è§‰ + 20æ–‡æœ¬ + 43åŠ¨ä½œ = 256 tokens
  â†“
LLM Layers (32å±‚)
  â”œâ†’ Layer 0-4: æ ‡å‡†å±‚ (256 tokens)
  â”œâ†’ Layer 5: LFP (å‰ªåˆ°227 tokens)
  â”œâ†’ Layer 10: LFP (å‰ªåˆ°208 tokens)
  â”œâ†’ Layer 15: LFP (å‰ªåˆ°179 tokens)
  â”œâ†’ Layer 20: LFP (å‰ªåˆ°141 tokens)
  â”œâ†’ Layer 25: LFP (å‰ªåˆ°121 tokens)
  â”œâ†’ Layer 30: LFP (å‰ªåˆ°102 tokens)
  â””â†’ Layer 31: æ ‡å‡†å±‚
     â†“ (æ¯å±‚çš„SDPAéƒ½ç”¨äº†Parallel Action Chunking)
  â†“
LM Head â†’ Logits [B,256,vocab_size]
  â†“
Cross Entropy Loss
  â†“
Backward (æ›´æ–°LoRA + MoE + LFP + Aggr Tokens)
```

### å…³é”®è®¾è®¡æƒè¡¡æ€»ç»“

| é˜¶æ®µ | è®¾è®¡ç‚¹ | åŸå›  |
|------|--------|------|
| **é…ç½®æ³¨å…¥** | ç”¨`vla_cfg`ä¼ é€’å¼€å…³ | è®©AutoModelè‡ªåŠ¨åˆå§‹åŒ–å¯¹åº”ç»„ä»¶ |
| **LoRAæ’é™¤** | æ’é™¤MoE/LFP/FiLM | è¿™äº›å°æ¨¡å—éœ€è¦å…¨é‡è®­ç»ƒï¼ŒLoRAä¸é€‚åˆ |
| **Wrapperè¿è¡Œæ—¶æ³¨å…¥** | è®­ç»ƒå‰åŒ…è£¹backbone | baseæƒé‡å·²freezeï¼Œåªèƒ½åœ¨å¤–é¢åŠ wrapper |
| **Visionåˆ†å¼€ä¿å­˜** | `vision_backbone--*.pt` | Wrapperä¸åœ¨base modelé‡Œï¼Œmergeæ—¶æ¼æ‰ |
| **ä¸‰æ­¥åŠ è½½** | merged + wrapper + stats | å…¼å®¹ç¦»çº¿merge workflow |

---

## ğŸ’¡ å®è·µå»ºè®®

### 1. è°ƒè¯•ç­–ç•¥

ä»ç®€å•åˆ°å¤æ‚ï¼Œé€æ­¥å¯ç”¨ä¼˜åŒ–ï¼š

```bash
# Step 1: åŸºç¡€è®­ç»ƒï¼ˆç¡®ä¿æµç¨‹èƒ½è·‘é€šï¼‰
--use_film False --use_aggr False --use_lfp False

# Step 2: æ·»åŠ Aggregation Tokensï¼ˆæœ€æ˜¾è‘—çš„ä¼˜åŒ–ï¼‰
--use_aggr True --num_vision_aggr 64

# Step 3: æ·»åŠ FiLMï¼ˆæå‡ç‰¹å¾è´¨é‡ï¼‰
--use_film True

# Step 4: æ·»åŠ MoE Routerï¼ˆæ™ºèƒ½èåˆï¼‰
--vision_aggregate_type moe

# Step 5: æ·»åŠ LFPï¼ˆæ¨ç†åŠ é€Ÿï¼‰
--use_lfp True --lfp_enable_film True
```

### 2. ç›‘æ§æŒ‡æ ‡

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§è¿™äº›å…³é”®æŒ‡æ ‡ï¼š

- **MoE Routeræƒé‡åˆ†å¸ƒ**ï¼šè§‚å¯ŸSigLIP vs DINOv2çš„æƒé‡å˜åŒ–
- **LFPä¿ç•™ç‡**ï¼šæ¯å±‚å®é™…ä¿ç•™çš„è§†è§‰tokenæ¯”ä¾‹
- **Aggregation Tokensæ¢¯åº¦**ï¼šç¡®ä¿èšåˆtokenåœ¨å­¦ä¹ 
- **è®­ç»ƒé€Ÿåº¦**ï¼šå¯¹æ¯”å¯ç”¨å‰åçš„iteration time

### 3. å†…å­˜ä¼˜åŒ–é¡ºåº

å¦‚æœé‡åˆ°OOMï¼ŒæŒ‰ç…§è¿™ä¸ªé¡ºåºå¯ç”¨ä¼˜åŒ–ï¼š

1. **å…ˆç”¨Aggregation Tokens**ï¼ˆå‡å°‘åºåˆ—é•¿åº¦ï¼Œæ•ˆæœæœ€æ˜¾è‘—ï¼‰
2. **å†åŠ LFP**ï¼ˆè¿›ä¸€æ­¥å‡å°‘è®¡ç®—é‡ï¼‰
3. **æœ€åè€ƒè™‘MoE Router**ï¼ˆç•¥å¾®å¢åŠ å‚æ•°é‡ï¼Œä½†æå‡è´¨é‡ï¼‰

### 4. å¸¸è§é—®é¢˜æ’æŸ¥

| é—®é¢˜ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|---------|---------|
| æ¨ç†æ—¶æ‰¾ä¸åˆ°`vision_backbone--*.pt` | è®­ç»ƒæ—¶æ²¡ä¿å­˜wrapper | æ£€æŸ¥`use_aggr`æˆ–`use_film`æ˜¯å¦ä¸ºTrue |
| `non_lora_trainables`åŠ è½½å¤±è´¥ | ç¦»çº¿mergeæ—¶æ¼åŠ è½½ | ç¡®ä¿mergeè„šæœ¬åŠ è½½äº†è¿™ä¸ªæ–‡ä»¶ |
| LFPæŠ¥é”™ä¸æ”¯æŒflash-attn-2 | LFPä¸flash-attnå†²çª | è®¾ç½®`attn_implementation='sdpa'` |
| MoE Routeræƒé‡å…¨æ˜¯NaN | å­¦ä¹ ç‡è¿‡å¤§ | é™ä½å­¦ä¹ ç‡æˆ–å¢åŠ warmup steps |

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [LFP_mechanism.md](./LFP_mechanism.md) - LFPå‰ªææœºåˆ¶å®Œæ•´è¯¦è§£ï¼ˆæ‰“åˆ†ã€é€‰æ‹©ã€å‹ç¼©ã€åŠ é€ŸåŸç†ï¼‰
- [vision_backbone_flow.md](./vision_backbone_flow.md) - Vision Backboneæ‰§è¡Œæµç¨‹ï¼ˆAggregation Tokensä¸FiLMè°ƒåˆ¶ï¼‰
- [vision_language_interaction_flow.md](./vision_language_interaction_flow.md) - è§†è§‰-è¯­è¨€äº¤äº’æœºåˆ¶è¯¦è§£
- [CogVLA_INTEGRATION.md](./CogVLA_INTEGRATION.md) - æ¨¡å—å®ç°ç»†èŠ‚å’Œé›†æˆæŒ‡å—
- [è®­ç»ƒè„šæœ¬ç¤ºä¾‹](../scripts-sh/finetune.sh) - å®Œæ•´çš„è®­ç»ƒå‘½ä»¤å‚è€ƒ
- [æ¨ç†è„šæœ¬ç¤ºä¾‹](../scripts-sh/eval_aloha_deploy.sh) - éƒ¨ç½²å’Œæ¨ç†ç¤ºä¾‹

---

**æœ¬æ–‡æ¡£ç‰ˆæœ¬**ï¼š2025-12-23  
**é€‚ç”¨CogVLAç‰ˆæœ¬**ï¼šåŸºäºOpenVLA-7Bçš„å®ç°
