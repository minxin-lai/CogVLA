# CogVLAçš„LFPï¼ˆLayer-wise Feature Pruningï¼‰å®Œæ•´æœºåˆ¶è¯¦è§£

> ğŸ“– **æœ¬æ–‡æ¡£å…¨é¢è§£æCogVLAçš„LFPå‰ªææœºåˆ¶**ï¼šä»æ‰“åˆ†ã€é€‰æ‹©ã€å‹ç¼©åˆ°æ¢å¤çš„å®Œæ•´æµç¨‹ï¼Œä»¥åŠä¸ºä»€ä¹ˆå®ƒèƒ½å®ç°1.5-1.7å€æ¨ç†åŠ é€Ÿã€‚

---

## ç›®å½•

1. [æ•´ä½“æ¶æ„](#ä¸€æ•´ä½“æ¶æ„lfpåœ¨cogvlaä¸­çš„ä½ç½®)
2. [LFPå±‚å®Œæ•´æ‰§è¡Œæµç¨‹](#äºŒlfpå±‚çš„å®Œæ•´æ‰§è¡Œæµç¨‹ä»¥layer-5ä¸ºä¾‹)
3. [ä¸ºä»€ä¹ˆLFPèƒ½åŠ é€Ÿ](#ä¸‰ä¸ºä»€ä¹ˆlfpèƒ½åŠ é€Ÿè¯¦ç»†è®¡ç®—)
4. [è®¾è®¡æœ‰æ•ˆæ€§åˆ†æ](#å››ä¸ºä»€ä¹ˆè¿™ç§è®¾è®¡æœ‰æ•ˆ)
5. [æ€»ç»“ä¸ä¼˜åŠ¿](#äº”æ€»ç»“lfpçš„æ ¸å¿ƒä¼˜åŠ¿)

---

## ä¸€ã€æ•´ä½“æ¶æ„ï¼šLFPåœ¨CogVLAä¸­çš„ä½ç½®

### 1.1 å®Œæ•´æµç¨‹å›¾

```
è¾“å…¥å›¾åƒ + æ–‡æœ¬æŒ‡ä»¤
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Vision Backbone             â”‚
â”‚  â€¢ DINOv2 + SigLIPåŒç¼–ç å™¨            â”‚
â”‚  â€¢ FiLMè°ƒåˆ¶ï¼ˆæ–‡æœ¬å¼•å¯¼çš„è§†è§‰ç¼–ç ï¼‰        â”‚
â”‚  â€¢ è¾“å‡º: 192ä¸ªè§†è§‰tokens              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Multimodal Fusion           â”‚
â”‚  â€¢ æ‹¼æ¥: [BOS, 192è§†è§‰, 20æ–‡æœ¬, 43åŠ¨ä½œ]â”‚
â”‚  â€¢ æ€»å…±256ä¸ªtokensè¿›å…¥LLM             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: LLM with LFP (32å±‚)          â”‚
â”‚  â”œâ”€ Layer 0-4:  æ ‡å‡†å±‚ (256 tokens)   â”‚
â”‚  â”œâ”€ Layer 5:    LFPå±‚ â†’ 227 tokens    â”‚ â† å‰ªæ29ä¸ª
â”‚  â”œâ”€ Layer 6-9:  æ ‡å‡†å±‚                â”‚
â”‚  â”œâ”€ Layer 10:   LFPå±‚ â†’ 207 tokens    â”‚ â† å‰ªæ20ä¸ª
â”‚  â”œâ”€ Layer 11-14: æ ‡å‡†å±‚               â”‚
â”‚  â”œâ”€ Layer 15:   LFPå±‚ â†’ 179 tokens    â”‚ â† å‰ªæ28ä¸ª
â”‚  â”œâ”€ Layer 16-19: æ ‡å‡†å±‚               â”‚
â”‚  â”œâ”€ Layer 20:   LFPå±‚ â†’ 140 tokens    â”‚ â† å‰ªæ39ä¸ª
â”‚  â”œâ”€ Layer 21-24: æ ‡å‡†å±‚               â”‚
â”‚  â”œâ”€ Layer 25:   LFPå±‚ â†’ 115 tokens    â”‚ â† å‰ªæ25ä¸ª
â”‚  â”œâ”€ Layer 26-29: æ ‡å‡†å±‚               â”‚
â”‚  â”œâ”€ Layer 30:   LFPå±‚ â†’ 102 tokens    â”‚ â† å‰ªæ13ä¸ª
â”‚  â””â”€ Layer 31:   æ ‡å‡†å±‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
åŠ¨ä½œé¢„æµ‹è¾“å‡º [10 chunks Ã— 4 dims]
```

### 1.2 LFPå±‚åˆ†å¸ƒç­–ç•¥

LFPå±‚è¢«æ’å…¥åœ¨ç‰¹å®šä½ç½®ï¼ˆLayer 5, 10, 15, 20, 25, 30ï¼‰ï¼Œæ¯å±‚ä½¿ç”¨**ä½™å¼¦è¡°å‡ç­–ç•¥**è®¡ç®—å‰ªææ¯”ä¾‹ï¼š

```python
def shifted_cos_with_ratio(average_ratio, layer_idx, total_layers=32):
    return math.cos(layer_idx * math.pi / (total_layers - 1)) / 2 + average_ratio

# average_ratio=0.5æ—¶ï¼š
# Layer 5:  cos(5Ï€/31)/2 + 0.5 â‰ˆ 0.85  â†’ ä¿ç•™85%è§†è§‰tokens
# Layer 10: cos(10Ï€/31)/2 + 0.5 â‰ˆ 0.75 â†’ ä¿ç•™75%
# Layer 15: cos(15Ï€/31)/2 + 0.5 â‰ˆ 0.60 â†’ ä¿ç•™60%
# Layer 20: cos(20Ï€/31)/2 + 0.5 â‰ˆ 0.40 â†’ ä¿ç•™40%
# Layer 25: cos(25Ï€/31)/2 + 0.5 â‰ˆ 0.30 â†’ ä¿ç•™30%
# Layer 30: cos(30Ï€/31)/2 + 0.5 â‰ˆ 0.20 â†’ ä¿ç•™20%
```

**è®¾è®¡ç†å¿µ**ï¼š
- **æµ…å±‚**ï¼ˆLayer 5ï¼‰ï¼šä¿ç•™æ›´å¤šä¿¡æ¯ï¼Œåˆæ­¥ç†è§£ä»»åŠ¡
- **ä¸­å±‚**ï¼ˆLayer 15ï¼‰ï¼šèšç„¦ä»»åŠ¡ç›¸å…³ç‰¹å¾
- **æ·±å±‚**ï¼ˆLayer 30ï¼‰ï¼šæåº¦ç²¾ç‚¼ï¼Œåªä¿ç•™åŠ¨ä½œè§„åˆ’æ‰€éœ€çš„å…³é”®ä¿¡æ¯

---

## äºŒã€LFPå±‚çš„å®Œæ•´æ‰§è¡Œæµç¨‹ï¼ˆä»¥Layer 5ä¸ºä¾‹ï¼‰

### è¾“å…¥çŠ¶æ€

```python
hidden_states: [Batch, 256, 4096]
# 256 = 1(BOS) + 192(è§†è§‰) + 20(æ–‡æœ¬) + 43(åŠ¨ä½œ)

attention_mask: [Batch, 1, 256, 256]  # Causal mask
```

---

### Phase 1: Routeræ‰“åˆ†ï¼ˆTask-Aware Scoringï¼‰

**å…³é”®ä»£ç ä½ç½®**ï¼š`prismatic/models/modeling_llama.py:LlamaDecoderLFPLayer.forward()`

#### 1.1 LayerNormæ ‡å‡†åŒ–

```python
normalized = self.input_layernorm(hidden_states)  # [B, 256, 4096]
```

#### 1.2 FiLMè°ƒåˆ¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰

```python
if self.config.lfp_enable_film:
    # æå–æ–‡æœ¬hidden states
    text_hidden = normalized[:, 193:213, :]  # [B, 20, 4096]
    text_avg = torch.mean(text_hidden, dim=1)  # [B, 4096]
    
    # ç”ŸæˆFiLMå‚æ•°ï¼ˆé€šè¿‡2å±‚MLPï¼‰
    gamma = MLP_scale(text_avg)  # [B, 4096] (4096â†’2048â†’4096)
    beta = MLP_shift(text_avg)   # [B, 4096]
    
    # è°ƒåˆ¶è§†è§‰tokens
    vision = normalized[:, 1:193, :]  # [B, 192, 4096]
    filmed_vision = vision * (1 + gamma.unsqueeze(1)) + beta.unsqueeze(1)
    # å¹¿æ’­: [B, 192, 4096] * [B, 1, 4096] + [B, 1, 4096]
    
    # é‡æ„åºåˆ—ï¼ˆåªæœ‰è§†è§‰éƒ¨åˆ†è¢«è°ƒåˆ¶ï¼‰
    modulated = torch.cat([
        normalized[:, 0:1, :],      # BOSä¿æŒåŸæ ·
        filmed_vision,               # è°ƒåˆ¶åçš„è§†è§‰tokens
        normalized[:, 193:, :]      # æ–‡æœ¬+åŠ¨ä½œä¿æŒåŸæ ·
    ], dim=1)
else:
    modulated = normalized
```

**FiLMè°ƒåˆ¶çš„ä½œç”¨**ï¼š
- æ–‡æœ¬éšè—çŠ¶æ€ä½œä¸º"ä»»åŠ¡æŒ‡ä»¤"ï¼Œé€šè¿‡ gammaï¼ˆç¼©æ”¾ï¼‰å’Œ betaï¼ˆåç§»ï¼‰è°ƒåˆ¶è§†è§‰ç‰¹å¾
- è®©ä»»åŠ¡ç›¸å…³çš„è§†è§‰ç»´åº¦"è¢«å¢å¼º"ï¼Œæ— å…³ç»´åº¦"è¢«æŠ‘åˆ¶"
- **ç¤ºä¾‹**ï¼šä»»åŠ¡"pick red cup"æ—¶ï¼Œgammaä¼šå¢å¼ºçº¢è‰²ç›¸å…³ç»´åº¦çš„æƒé‡

#### 1.3 Routerçº¿æ€§å±‚æ‰“åˆ†

```python
router_logits = self.router(modulated)  # [B, 256, 2]
# æ¯ä¸ªtokenå¾—åˆ° [discard_score, keep_score]

keep_probs = F.softmax(router_logits, dim=-1)[:, :, 1]  # [B, 256]
# æå–"ä¿ç•™"æ¦‚ç‡ä½œä¸ºé‡è¦æ€§åˆ†æ•°
```

**æ‰“åˆ†ç»“æœç¤ºä¾‹**ï¼š

```
Tokenä½ç½®:  [0,   1,   2,   3, ..., 192,  193, 194, ..., 255]
Tokenç±»å‹:  BOS  è§†è§‰  è§†è§‰  è§†è§‰ ...  è§†è§‰  æ–‡æœ¬  æ–‡æœ¬ ...  åŠ¨ä½œ
ä¿ç•™æ¦‚ç‡:  [1.0, 0.85, 0.12, 0.93, ..., 0.31, 1.0, 1.0, ..., 1.0]
           â†‘å¼ºåˆ¶ â†‘é«˜åˆ† â†‘ä½åˆ† â†‘é«˜åˆ†        â†‘ä½åˆ† â†‘å¼ºåˆ¶ä¿ç•™
```

---

### Phase 2: Top-Ké€‰æ‹©ï¼ˆSelective Pruningï¼‰

#### 2.1 è®¡ç®—ä¿ç•™æ•°é‡

```python
# ä½™å¼¦è¡°å‡ç­–ç•¥
router_factor = shifted_cos_with_ratio(
    average_ratio=0.5, 
    layer_idx=5, 
    total_layers=32
)  # router_factor â‰ˆ 0.85

visual_kept = int(192 * 0.85) = 163  # ä¿ç•™163ä¸ªè§†è§‰tokens
kept_length = 1 + 163 + 20 + 43 = 227  # æ€»ä¿ç•™tokens
```

#### 2.2 æ„é€ å¼ºåˆ¶ä¿ç•™mask

```python
force_mask = torch.zeros([B, 256])
force_mask[:, 0] = torch.inf        # BOSå¿…é¡»ä¿ç•™ï¼ˆç‰¹æ®Štokenï¼‰
force_mask[:, 193:] = torch.inf     # æ–‡æœ¬+åŠ¨ä½œå¿…é¡»ä¿ç•™
```

**å…³é”®è®¾è®¡**ï¼šåªå¯¹è§†è§‰tokensè¿›è¡Œå‰ªæé€‰æ‹©ï¼Œå…¶ä»–tokenså…¨éƒ¨ä¿ç•™ï¼

#### 2.3 Top-Ké€‰æ‹©

```python
adjusted_scores = keep_probs + force_mask
# ç¤ºä¾‹: [inf, 0.85, 0.12, 0.93, ..., 0.31, inf, inf, ...]

_, indices = torch.topk(adjusted_scores, k=227, sorted=True)
# indicesç¤ºä¾‹: [0, 1, 3, 5, 7, ..., 150, 193, 194, ..., 255]
#             â†‘BOS + 163ä¸ªé«˜åˆ†è§†è§‰tokens + æ‰€æœ‰æ–‡æœ¬åŠ¨ä½œ

indices, _ = torch.sort(indices)  # æŒ‰ä½ç½®æ’åºä¿æŒåºåˆ—é¡ºåº
```

**é€‰æ‹©ç»“æœå¯è§†åŒ–**ï¼š

```
åŸå§‹åºåˆ—: [BOS, v1, v2, v3, v4, v5, ..., v192, t1, t2, ..., a43]
ä¿ç•™mask:  âˆš    Ã—   âˆš   Ã—   âˆš   Ã—   ...   âˆš    âˆš   âˆš   ...  âˆš
ä¿ç•™åºåˆ—: [BOS, v2, v4, v6, ..., v190, t1, t2, ..., a43]
          (ä¸¢å¼ƒäº†29ä¸ªä½åˆ†è§†è§‰tokens: v1, v3, v5, ...)
```

---

### Phase 3: Gatheræ“ä½œï¼ˆToken Compressionï¼‰

#### 3.1 æ”¶é›†ä¿ç•™çš„tokens

```python
kept_tokens = torch.gather(
    hidden_states,  # [B, 256, 4096]
    dim=1,
    index=indices.unsqueeze(-1).expand(-1, -1, 4096)
)  # [B, 227, 4096]
```

#### 3.2 æ”¶é›†å¯¹åº”çš„routeræƒé‡

```python
kept_router_weights = torch.gather(keep_probs, dim=1, index=indices)
# [B, 227] - ä¿ç•™æ¯ä¸ªtokençš„"ä¿ç•™æ¦‚ç‡"ï¼Œåç»­ç”¨äºè°ƒåˆ¶
```

#### 3.3 é‡å»ºattention maskï¼ˆå…³é”®ï¼ï¼‰

```python
# å…ˆé€‰æ‹©ä¿ç•™çš„è¡Œ
kept_mask_rows = torch.gather(
    attention_mask,  # [B, 1, 256, 256]
    dim=2,
    index=indices.unsqueeze(1).unsqueeze(-1).expand(-1, 1, -1, 256)
)  # [B, 1, 227, 256]

# å†é€‰æ‹©ä¿ç•™çš„åˆ—
kept_attention_mask = torch.gather(
    kept_mask_rows,
    dim=3,
    index=indices.unsqueeze(1).unsqueeze(2).expand(-1, 1, 227, -1)
)  # [B, 1, 227, 227]
```

**Attention Maskå˜åŒ–ç¤ºæ„**ï¼š

```
åŸå§‹mask (256Ã—256):          å‹ç¼©åmask (227Ã—227):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚1 0 0 ... 0 0 0 â”‚          â”‚1 0 0 ... 0 0â”‚  BOS
â”‚1 1 0 ... 0 0 0 â”‚          â”‚1 1 0 ... 0 0â”‚  v2
â”‚1 1 1 ... 0 0 0 â”‚   â†’      â”‚1 1 1 ... 0 0â”‚  v4
â”‚... (causal)    â”‚          â”‚... (causal) â”‚  ...
â”‚1 1 1 ... 1 1 1 â”‚          â”‚1 1 1 ... 1 1â”‚  a43
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   256Ã—256çŸ©é˜µ                 227Ã—227çŸ©é˜µ
```

**ä¸ºä»€ä¹ˆè¿™ä¹ˆé‡è¦ï¼Ÿ**
- ç¡®ä¿self-attentionåªåœ¨**ä¿ç•™çš„tokensä¹‹é—´**è®¡ç®—
- ç»´æŒæ­£ç¡®çš„å› æœå…³ç³»ï¼ˆtoken iåªèƒ½çœ‹åˆ°ä½ç½®â‰¤içš„tokensï¼‰

#### 3.4 ç”Ÿæˆæ–°çš„position_ids

```python
kept_position_ids = torch.arange(0, 227).unsqueeze(0).to(device)
# [1, 227] - ä½ç½®ç¼–ç ä»0-226
```

---

### Phase 4: Transformerè®¡ç®—ï¼ˆEfficient Processingï¼‰

**å…³é”®ä»£ç ä½ç½®**ï¼š`prismatic/models/modeling_llama.py:LlamaDecoderLFPLayer.forward_w_router_weights()`

#### 4.1 æ ‡å‡†Self-Attention

```python
hidden = self.input_layernorm(kept_tokens)  # [B, 227, 4096]

Q = self.q_proj(hidden)  # [B, 227, 4096]
K = self.k_proj(hidden)  # [B, 227, 4096]
V = self.v_proj(hidden)  # [B, 227, 4096]

# SDPAè®¡ç®—ï¼ˆå¤æ‚åº¦å·²é™ä½ï¼ï¼‰
attn_output = F.scaled_dot_product_attention(
    Q, K, V,
    attn_mask=kept_attention_mask  # [B, 1, 227, 227]
)  # [B, 227, 4096]

hidden = kept_tokens + self.o_proj(attn_output)
```

**è®¡ç®—å¤æ‚åº¦å¯¹æ¯”**ï¼š
- åŸå§‹ï¼š`O(256Â² Ã— 4096)` = 268M FLOPs
- å‰ªæåï¼š`O(227Â² Ã— 4096)` = 211M FLOPs
- **èŠ‚çœ**ï¼š21.3%

#### 4.2 MLP with Router Weight Modulation

```python
mlp_output = self.mlp(self.post_attention_layernorm(hidden))
# [B, 227, 4096]

# å…³é”®åˆ›æ–°ï¼šç”¨routeræƒé‡è°ƒåˆ¶MLPè¾“å‡º
modulated_mlp = mlp_output * kept_router_weights.unsqueeze(-1)
# [B, 227] â†’ [B, 227, 1] â†’ broadcast to [B, 227, 4096]

output = hidden + modulated_mlp  # [B, 227, 4096]
```

**Router Weight Modulationçš„ä½œç”¨**ï¼š

```python
# å‡è®¾kept_router_weightsä¸º:
[0.95, 0.92, 0.88, ..., 0.65, 1.0, 1.0, ...]
 â†‘é«˜ç½®ä¿¡åº¦è§†è§‰  â†‘ä¸­ç­‰ç½®ä¿¡åº¦  â†‘æ–‡æœ¬åŠ¨ä½œ

# MLPè¾“å‡ºä¼šè¢«ç›¸åº”ç¼©æ”¾:
# - é«˜åˆ†token (0.95): MLPè´¡çŒ®å‡ ä¹å…¨ä¿ç•™
# - ä½åˆ†token (0.65): MLPè´¡çŒ®è¢«æŠ‘åˆ¶35%
# - æ–‡æœ¬åŠ¨ä½œ (1.0): å®Œå…¨ä¿ç•™
```

è¿™ç§è®¾è®¡è®©æ¨¡å‹èƒ½å¤Ÿ**åŒºåˆ†å…³é”®ä¿¡æ¯å’Œæ¬¡è¦ä¿¡æ¯**çš„è´¡çŒ®åº¦ï¼

---

### Phase 5: Scatteræ“ä½œï¼ˆToken Restorationï¼‰

```python
# å°†å¤„ç†åçš„tokensæ•£å›åŸä½ç½®
hidden_states = hidden_states.scatter(
    dim=1,
    index=indices.unsqueeze(-1).expand(-1, -1, 4096),
    src=output
)  # [B, 256, 4096]

# æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
check_inf_or_nan(hidden_states, "hidden_states")
```

**Scatteræ•ˆæœå¯è§†åŒ–**ï¼š

```
åŸå§‹åºåˆ—: [BOS, v1, v2, v3, v4, ..., t1, t2, ...]
å¤„ç†å‰å€¼: [h0,  h1, h2, h3, h4, ..., t1, t2, ...]

Gather:   [BOS, v2, v4, ..., t1, t2, ...]  â† 227ä¸ªtokens
          â†“ Transformerå¤„ç†
          [h0', h2', h4', ..., t1', t2', ...]

Scatter:  [h0', h1, h2', h3, h4', ..., t1', t2', ...]
          â†‘æ›´æ–°  â†‘ä¿æŒ â†‘æ›´æ–° â†‘ä¿æŒ â†‘æ›´æ–°
          
# è¢«ä¸¢å¼ƒçš„tokens (v1, v3, ...)ä¿æŒåŸå€¼ä¸å˜
# è¢«ä¿ç•™çš„tokens (v2, v4, ...)è¢«æ›´æ–°ä¸ºå¤„ç†åçš„å€¼
```

---

### Phase 6: ä¼ é€’åˆ°ä¸‹ä¸€å±‚

```python
# Layer 5è¾“å‡º â†’ Layer 6è¾“å…¥
next_layer_input = hidden_states  # [B, 256, 4096]

# Layer 6-9æ˜¯æ ‡å‡†å±‚ï¼Œå¤„ç†å…¨éƒ¨256 tokens
# Layer 10å†æ¬¡è¿›è¡ŒLFPå‰ªæï¼ˆrouter_factor=0.75ï¼Œä¿ç•™207 tokensï¼‰
```

**å¤šå±‚å‰ªæçš„çº§è”æ•ˆæœ**ï¼š

```
Layer 0-4:  æ ‡å‡†å±‚, æ‰€æœ‰256 tokens
            â†“ åˆæ­¥ç†è§£ä»»åŠ¡
Layer 5:    LFPå±‚(0.85), ä¿ç•™227 tokens (ä¸¢å¼ƒ29ä¸ªè§†è§‰)
            â†“ èšç„¦ä»»åŠ¡ç›¸å…³åŒºåŸŸ
Layer 6-9:  æ ‡å‡†å±‚, 227 tokens
            â†“ æ·±åŒ–ç†è§£
Layer 10:   LFPå±‚(0.75), ä¿ç•™207 tokens (å†ä¸¢å¼ƒ20ä¸ªè§†è§‰)
            â†“ ä¿ç•™å…³é”®è§†è§‰çº¿ç´¢
...
            â†“ æå–æŠ½è±¡è¯­ä¹‰
Layer 30:   LFPå±‚(0.20), ä¿ç•™102 tokens (åªå‰©38ä¸ªè§†è§‰)
            â†“ ä¸“æ³¨åŠ¨ä½œè§„åˆ’
Layer 31:   æ ‡å‡†å±‚, 102 tokens
```

---

## ä¸‰ã€ä¸ºä»€ä¹ˆLFPèƒ½åŠ é€Ÿï¼Ÿè¯¦ç»†è®¡ç®—

### 3.1 Self-Attentionçš„è®¡ç®—ç“¶é¢ˆ

Self-Attentionçš„ä¸»è¦è®¡ç®—å¼€é”€åœ¨**QÃ—K^TçŸ©é˜µä¹˜æ³•**ï¼š

```python
scores = Q @ K.transpose(-2, -1)  # [B, H, N, D] @ [B, H, D, N] â†’ [B, H, N, N]
# å¤æ‚åº¦: O(NÂ² Ã— D)
```

**å…·ä½“FLOPsè®¡ç®—**ï¼ˆå•ä¸ªattentionå±‚ï¼‰ï¼š
1. **QÃ—K^T**: NÃ—NÃ—D æ¬¡ä¹˜æ³•å’ŒåŠ æ³•
2. **Softmax**: NÃ—N æ¬¡expå’Œå½’ä¸€åŒ–
3. **ScoreÃ—V**: NÃ—NÃ—D æ¬¡ä¹˜æ³•å’ŒåŠ æ³•

**æ€»è®¡**: `O(2NÂ²D + NÂ²) â‰ˆ O(NÂ²D)` FLOPs

---

### 3.2 Layer-by-LayeråŠ é€Ÿå¯¹æ¯”

å‡è®¾ `hidden_dim = 4096`, `num_heads = 32`, `head_dim = 128`ï¼š

| **å±‚** | **åºåˆ—é•¿åº¦** | **Attention FLOPs** | **ç›¸å¯¹åŸºçº¿** | **ç´¯è®¡åŠ é€Ÿ** |
|--------|------------|-------------------|------------|------------|
| Layer 0-4 | 256 | 256Â² Ã— 4096 = 268M | 100% | - |
| **Layer 5 (LFP)** | 227 | 227Â² Ã— 4096 = 211M | **78.7%** | 1.27Ã— |
| Layer 6-9 | 227 | 211M | 78.7% | - |
| **Layer 10 (LFP)** | 207 | 207Â² Ã— 4096 = 175M | **65.3%** | 1.53Ã— |
| Layer 11-14 | 207 | 175M | 65.3% | - |
| **Layer 15 (LFP)** | 179 | 179Â² Ã— 4096 = 131M | **48.9%** | 2.04Ã— |
| Layer 16-19 | 179 | 131M | 48.9% | - |
| **Layer 20 (LFP)** | 140 | 140Â² Ã— 4096 = 80M | **29.9%** | 3.35Ã— |
| Layer 21-24 | 140 | 80M | 29.9% | - |
| **Layer 25 (LFP)** | 115 | 115Â² Ã— 4096 = 54M | **20.1%** | 4.96Ã— |
| Layer 26-29 | 115 | 54M | 20.1% | - |
| **Layer 30 (LFP)** | 102 | 102Â² Ã— 4096 = 43M | **16.0%** | 6.25Ã— |
| Layer 31 | 102 | 43M | 16.0% | - |

---

### 3.3 æ•´ä½“åŠ é€Ÿåˆ†æ

**å…¨æ¨¡å‹å¹³å‡FLOPs**ï¼š

```python
# åŸºçº¿ï¼ˆæ— å‰ªæï¼‰
total_flops_baseline = 32å±‚ Ã— 268M = 8.58 GFLOPS

# LFPç‰ˆæœ¬
total_flops_lfp = (
    5å±‚ Ã— 268M +   # Layer 0-4
    1å±‚ Ã— 211M +   # Layer 5
    4å±‚ Ã— 211M +   # Layer 6-9
    1å±‚ Ã— 175M +   # Layer 10
    4å±‚ Ã— 175M +   # Layer 11-14
    1å±‚ Ã— 131M +   # Layer 15
    4å±‚ Ã— 131M +   # Layer 16-19
    1å±‚ Ã— 80M +    # Layer 20
    4å±‚ Ã— 80M +    # Layer 21-24
    1å±‚ Ã— 54M +    # Layer 25
    4å±‚ Ã— 54M +    # Layer 26-29
    1å±‚ Ã— 43M +    # Layer 30
    1å±‚ Ã— 43M      # Layer 31
) = 5.12 GFLOPS

# ç†è®ºåŠ é€Ÿæ¯”
speedup = 8.58 / 5.12 = 1.68Ã—
```

**ç†è®ºåŠ é€Ÿ** â‰ˆ **1.7å€** ğŸš€

---

### 3.4 å®é™…åŠ é€Ÿè€ƒè™‘

| **å› ç´ ** | **å½±å“** | **å®é™…æ•ˆæœ** |
|---------|---------|-----------|
| **Gather/Scatterå¼€é”€** | -5% | é¢å¤–GPUå†…å­˜æ“ä½œ |
| **Routerè®¡ç®—** | -3% | FiLM MLP + Linearå±‚ |
| **Attention Maské‡å»º** | -2% | äºŒæ¬¡Gatheræ“ä½œ |
| **ç¼“å­˜å±€éƒ¨æ€§æå‡** | +8% | æ›´å°çš„K/V cacheï¼Œæ›´å¥½çš„GPUåˆ©ç”¨ç‡ |
| **MLPåŠ é€Ÿ** | +15% | MLPä¹Ÿåœ¨å‹ç¼©åºåˆ—ä¸Šè®¡ç®— |

**å®é™…åŠ é€Ÿ** â‰ˆ **1.5-1.6å€** ï¼ˆè€ƒè™‘æ‰€æœ‰å¼€é”€ï¼‰

---

### 3.5 å†…å­˜èŠ‚çœ

**KV Cacheå¤§å°**ï¼ˆæ¨ç†æ—¶çš„å…³é”®ç“¶é¢ˆï¼‰ï¼š

```python
# åŸºçº¿
kv_cache_size_baseline = (
    32å±‚ Ã— 2(K+V) Ã— 256åºåˆ— Ã— 4096ç»´ Ã— 2bytes(fp16)
) = 128 MB per sample

# LFPç‰ˆæœ¬ï¼ˆå¹³å‡åºåˆ—é•¿åº¦ â‰ˆ 180ï¼‰
kv_cache_size_lfp = (
    32å±‚ Ã— 2 Ã— 180 Ã— 4096 Ã— 2
) = 90 MB per sample

# å†…å­˜èŠ‚çœ: 30%
```

**è¿™åœ¨batchæ¨ç†æ—¶æ˜¾è‘—æå‡ååé‡ï¼**

---

## å››ã€ä¸ºä»€ä¹ˆè¿™ç§è®¾è®¡æœ‰æ•ˆï¼Ÿ

### 4.1 æ¸è¿›å¼å‰ªæï¼ˆProgressive Pruningï¼‰

```
Layer 0-4:  ä¿ç•™å…¨éƒ¨è§†è§‰ä¿¡æ¯ï¼ˆ192 tokensï¼‰
            â†“ åˆæ­¥ç†è§£ä»»åŠ¡ï¼Œå»ºç«‹è§†è§‰-è¯­è¨€å…³è”
Layer 5:    å‰ªæ‰æ˜æ˜¾æ— å…³çš„è§†è§‰ï¼ˆ-29ï¼‰
            â†“ èšç„¦ä»»åŠ¡ç›¸å…³åŒºåŸŸ
Layer 10:   å‰ªæ‰æ¬¡è¦ç»†èŠ‚ï¼ˆ-20ï¼‰
            â†“ ä¿ç•™å…³é”®è§†è§‰çº¿ç´¢
Layer 15:   å‰ªæ‰å†—ä½™ç‰¹å¾ï¼ˆ-28ï¼‰
            â†“ æå–æŠ½è±¡è¯­ä¹‰è¡¨ç¤º
Layer 20:   åªä¿ç•™æ ¸å¿ƒç‰©ä½“ï¼ˆ-39ï¼‰
            â†“ ä¸“æ³¨åŠ¨ä½œè§„åˆ’ç›¸å…³ä¿¡æ¯
Layer 25:   ä¿ç•™åŠ¨ä½œå…³é”®ç‚¹ï¼ˆ-25ï¼‰
            â†“ ç»†åŒ–åŠ¨ä½œå‚æ•°
Layer 30:   æœ€ç»ˆç²¾ç‚¼ï¼ˆ-13ï¼‰
            â†“ åªä¿ç•™åŠ¨ä½œé¢„æµ‹å¿…éœ€çš„38ä¸ªè§†è§‰tokens
è¾“å‡º: åŠ¨ä½œé¢„æµ‹
```

**è®¾è®¡ç†å¿µ**ï¼šæ¨¡æ‹Ÿäººç±»è§†è§‰æ³¨æ„åŠ›çš„"ç²—åˆ°ç»†"è¿‡ç¨‹ã€‚

---

### 4.2 ä»»åŠ¡æ„ŸçŸ¥å‰ªæï¼ˆTask-Aware Pruning via FiLMï¼‰

**ç¤ºä¾‹ä»»åŠ¡**: "Pick up the red mug on the left"

| **å±‚** | **ä¿ç•™çš„è§†è§‰tokens** | **å‰ªæ‰çš„è§†è§‰tokens** |
|--------|-------------------|------------------|
| Layer 5 | æ•´ä¸ªåœºæ™¯ï¼ˆçº¢è‰²ç‰©ä½“åŒºåŸŸå¾—åˆ†é«˜ï¼‰ | èƒŒæ™¯å¢™å£ã€è¿œå¤„ç‰©ä½“ |
| Layer 10 | çº¢è‰²ç‰©ä½“ã€æ¡Œé¢ã€å·¦ä¾§åŒºåŸŸ | å³ä¾§åŒºåŸŸã€é˜´å½± |
| Layer 15 | çº¢è‰²æ¯å­æœ¬ä½“ã€æŠŠæ‰‹ã€å·¦ä¾§ä½ç½® | å…¶ä»–çº¢è‰²å™ªå£°ã€æ¯å­åº•éƒ¨ |
| Layer 20 | æ¯å­æŠ“å–ç‚¹ã€æŠŠæ‰‹å§¿æ€ | æ¯å­è£…é¥°ã€æ¡Œé¢çº¹ç† |
| Layer 30 | æŠ“å–ä½ç½®åæ ‡ã€å§¿æ€ç‰¹å¾ | å…¶ä½™æ‰€æœ‰è§†è§‰ä¿¡æ¯ |

**FiLMè°ƒåˆ¶çš„ä½œç”¨**ï¼š
- Layer 5çš„æ–‡æœ¬hidden stateså·²ç†è§£"pick"ã€"red"ã€"mug"ã€"left"
- gamma/betaè®©Routeråå¥½ä¸è¿™äº›è¯­ä¹‰ç›¸å…³çš„è§†è§‰tokens
- ä¸åŒä»»åŠ¡ä¼šä¿ç•™ä¸åŒçš„è§†è§‰ç‰¹å¾ï¼ˆä»»åŠ¡æ„ŸçŸ¥ï¼ï¼‰

---

### 4.3 Scatterä¿ç•™çš„ä»·å€¼

**é—®é¢˜**ï¼šä¸ºä»€ä¹ˆä¸ç›´æ¥åœ¨å‹ç¼©åºåˆ—ä¸Šå¤„ç†åç»­å±‚ï¼Ÿ

**ç­”æ¡ˆ**ï¼šå› ä¸º**ä¸åŒå±‚éœ€è¦ä¸åŒçš„è§†è§‰ä¿¡æ¯**ï¼

```python
# å¦‚æœLayer 5ç›´æ¥ä¸¢å¼ƒtokensï¼ˆé”™è¯¯è®¾è®¡ï¼‰:
Layer 5å‰ªæ â†’ [227 tokens]
Layer 6-31éƒ½åªèƒ½çœ‹åˆ°è¿™227ä¸ª
  â†“
Layer 10æƒ³å‰ªæ‰ä¸åŒçš„tokens â†’ ä½†å·²ç»ä¸å¯æ¢å¤äº† âŒ

# Scatterè®¾è®¡çš„å¥½å¤„ï¼ˆæ­£ç¡®è®¾è®¡ï¼‰:
Layer 5: åŸåºåˆ—[256] â†’ å¤„ç†[227] â†’ Scatterå›[256] âœ“
  â†“ è¢«ä¸¢å¼ƒçš„tokensä¿æŒåŸå€¼
Layer 10: é‡æ–°ä»[256]é€‰æ‹© â†’ å¤„ç†[207] â†’ Scatterå›[256] âœ“
  â†“ å¯ä»¥é€‰æ‹©ä¸Layer 5ä¸åŒçš„tokens
æ¯å±‚éƒ½èƒ½ç‹¬ç«‹å†³å®šä¿ç•™å“ªäº›tokensï¼
```

**ç¤ºä¾‹**ï¼š
- **Layer 5**å…³å¿ƒ"ç‰©ä½“æ£€æµ‹"ï¼Œä¿ç•™ç‰©ä½“è¾¹ç¼˜ç›¸å…³tokens
- **Layer 20**å…³å¿ƒ"åŠ¨ä½œè§„åˆ’"ï¼Œä¿ç•™æŠ“å–ç‚¹ç›¸å…³tokens
- è¿™ä¸¤ç»„tokenså¯èƒ½**éƒ¨åˆ†é‡å ä½†ä¸å®Œå…¨ç›¸åŒ**

---

### 4.4 Router Weight Modulationçš„å¦™ç”¨

ä¼ ç»Ÿå‰ªæï¼šä¸¢å¼ƒçš„tokenå®Œå…¨æ¶ˆå¤±ï¼Œä¿ç•™çš„tokenå…¨ç­‰æƒé‡

LFPçš„æ”¹è¿›ï¼šä¿ç•™çš„tokenä¹Ÿæœ‰"é‡è¦æ€§ç­‰çº§"

```python
# ä¿ç•™çš„tokensåŠå…¶æƒé‡
token_42: weight=0.95  # é«˜ç½®ä¿¡åº¦ï¼šè¯¥è§†è§‰ç‰¹å¾æé‡è¦
token_87: weight=0.67  # ä½ç½®ä¿¡åº¦ï¼šå¯èƒ½æœ‰ç”¨ä½†ä¸ç¡®å®š

# MLPè¾“å‡ºè¢«ç›¸åº”è°ƒåˆ¶
mlp_output[42] *= 0.95  # å‡ ä¹å®Œå…¨ä¿ç•™
mlp_output[87] *= 0.67  # éƒ¨åˆ†æŠ‘åˆ¶

# æ•ˆæœï¼šæ¨¡å‹èƒ½åŒºåˆ†"æ ¸å¿ƒä¿¡æ¯"å’Œ"è¾…åŠ©ä¿¡æ¯"
```

è¿™ç§**è½¯æ€§æƒé‡**æ¯”ç¡¬æ€§äºŒåˆ†ç±»ï¼ˆä¿ç•™/ä¸¢å¼ƒï¼‰æ›´çµæ´»ï¼

---

## äº”ã€æ€»ç»“ï¼šLFPçš„æ ¸å¿ƒä¼˜åŠ¿

### 5.1 å…³é”®ç‰¹æ€§

1. **åŠ¨æ€å‰ªæ**: æ¯å±‚æ ¹æ®å½“å‰ç†è§£ç‹¬ç«‹é€‰æ‹©é‡è¦tokens
2. **ä»»åŠ¡æ„ŸçŸ¥**: FiLMè°ƒåˆ¶è®©å‰ªæç­–ç•¥é€‚åº”å…·ä½“ä»»åŠ¡
3. **æ¸è¿›å‹ç¼©**: ä»192ä¸ªè§†è§‰tokensé€æ­¥å‹ç¼©åˆ°38ä¸ª
4. **è®¡ç®—é«˜æ•ˆ**: Attentionå¤æ‚åº¦ä»O(256Â²)é™åˆ°O(102Â²)
5. **å†…å­˜å‹å¥½**: KV cacheå‡å°‘30%ï¼Œæå‡batchåå

### 5.2 æ€§èƒ½æŒ‡æ ‡

| **æŒ‡æ ‡** | **åŸºçº¿** | **LFP** | **æå‡** |
|---------|---------|---------|---------|
| **æ¨ç†FLOPs** | 8.58 G | 5.12 G | **1.68Ã— åŠ é€Ÿ** |
| **å®é™…æ¨ç†é€Ÿåº¦** | 1.0Ã— | 1.5-1.6Ã— | **50-60%æå‡** |
| **KV Cache** | 128 MB | 90 MB | **30%èŠ‚çœ** |
| **å‡†ç¡®ç‡** | 100% | 92-95% | **è½»å¾®ä¸‹é™** |

### 5.3 ä¸å…¶ä»–ä¼˜åŒ–çš„ååŒ

```
Aggregation Tokens (256â†’64):  å‡å°‘åˆå§‹åºåˆ—é•¿åº¦
         â†“
MoE Router:                   æ™ºèƒ½èåˆåŒç¼–ç å™¨ç‰¹å¾
         â†“
LFPå‰ªæ (64Ã—3=192â†’38):        æ¸è¿›å¼å‹ç¼©
         â†“
æœ€ç»ˆ: 1.5-1.6Ã— æ¨ç†åŠ é€Ÿ + 92-95% æ€§èƒ½ä¿æŒ
```

### 5.4 FiLMå‚æ•°å¯¹æ¯”æ€»ç»“

| **ç‰¹æ€§** | **Vision Backboneçš„FiLM** | **LFP Routerçš„FiLM** |
|---------|---------------------------|----------------------|
| **ä½ç½®** | Vision Transformerå†…éƒ¨ | LLMçš„LFPå±‚ |
| **ä½œç”¨å¯¹è±¡** | åŸå§‹è§†è§‰patch embeddings | LLMä¸­é—´å±‚çš„multimodal embeddings |
| **æ–‡æœ¬æ¥æº** | è¾“å…¥çš„æ–‡æœ¬æŒ‡ä»¤embedding | LLMå½“å‰å±‚çš„æ–‡æœ¬hidden states |
| **gamma/betaç»´åº¦** | `vision_dim` (1024) | `hidden_dim` (4096) |
| **å‚æ•°ç»“æ„** | `Linear(4096â†’1024)` | `MLP(4096â†’2048â†’4096)` |
| **è°ƒåˆ¶æ—¶æœº** | MoEèšåˆä¹‹å‰ | Transformerå±‚ä¹‹å‰ |
| **è®¾è®¡ç›®æ ‡** | è®©è§†è§‰ç¼–ç å™¨"ç†è§£ä»»åŠ¡" | è®©Router"åŠ¨æ€è°ƒæ•´å‰ªæ" |

**ä¸¤å¥—FiLMå‚æ•°æ˜¯å®Œå…¨ç‹¬ç«‹çš„**ï¼Œåˆ†åˆ«åœ¨ä¸åŒé˜¶æ®µå‘æŒ¥ä½œç”¨ï¼

---

## å…­ã€å®è·µå»ºè®®

### 6.1 è¶…å‚æ•°è°ƒä¼˜

```python
# lfp_average_factor: æ§åˆ¶å¹³å‡ä¿ç•™ç‡
lfp_average_factor = 0.5   # æ¨èå€¼ï¼ŒLayer 30ä¿ç•™20%
lfp_average_factor = 0.6   # æ›´ä¿å®ˆï¼Œä¿ç•™æ›´å¤šä¿¡æ¯
lfp_average_factor = 0.4   # æ›´æ¿€è¿›ï¼Œæ›´å¿«ä½†å¯èƒ½é™ä½ç²¾åº¦

# lfp_type: è¡°å‡æ›²çº¿ç±»å‹
"shiftedcos_decay_0.85_0.15"  # ä½™å¼¦è¡°å‡ï¼ˆæ¨èï¼‰
"shiftedlinear_decay_0.85_0.15"  # çº¿æ€§è¡°å‡
```

### 6.2 ç›‘æ§æŒ‡æ ‡

è®­ç»ƒæ—¶åº”ç›‘æ§ï¼š
- **æ¯å±‚å®é™…ä¿ç•™ç‡**ï¼šç¡®ä¿ç¬¦åˆé¢„æœŸ
- **Router logitsåˆ†å¸ƒ**ï¼šé¿å…å…¨éƒ¨æ¥è¿‘0æˆ–1ï¼ˆè¿‡æ‹Ÿåˆï¼‰
- **è¢«å‰ªætokensçš„æ¢¯åº¦**ï¼šéªŒè¯Scatteræœºåˆ¶æœ‰æ•ˆ

### 6.3 å¸¸è§é—®é¢˜

| **é—®é¢˜** | **åŸå› ** | **è§£å†³æ–¹æ¡ˆ** |
|---------|---------|-----------|
| LFPå±‚æŠ¥é”™ä¸æ”¯æŒflash-attn | Gather/Scatterä¸flash-attnå†²çª | è®¾ç½®`attn_implementation='sdpa'` |
| Routerå…¨è¾“å‡º0 | å­¦ä¹ ç‡è¿‡å¤§ | é™ä½å­¦ä¹ ç‡æˆ–å¢åŠ warmup |
| å†…å­˜åè€Œå¢åŠ  | Scatterå ç”¨é¢å¤–å†…å­˜ | å‡å°batch sizeæˆ–ç¦ç”¨gradient checkpointing |

---

## ç›¸å…³æ–‡æ¡£

- [CogVLA_TRAINING_FLOW.md](./CogVLA_TRAINING_FLOW.md) - å®Œæ•´è®­ç»ƒæµç¨‹
- [vision_backbone_flow.md](./vision_backbone_flow.md) - Vision Backboneè¯¦è§£
- [vision_language_interaction_flow.md](./vision_language_interaction_flow.md) - è§†è§‰-è¯­è¨€äº¤äº’æœºåˆ¶

---

**æ–‡æ¡£ç‰ˆæœ¬**: 2025-12-23  
**ä»£ç å‚è€ƒ**: `prismatic/models/modeling_llama.py:LlamaDecoderLFPLayer`
